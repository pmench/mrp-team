---
title: "SI 699 Demo"
author: "Team 1"
date: "2024-03-012"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro to R 

```{r}
# install.packages('tidyverse')
# install.packages('lme4', type = 'source')
# install.packages("Matrix")
library(tidyverse)
library(lme4)
```

```{r}
load("dragons.RData")
head(dragons)
```

## Mixed Effects Regression
A mixed effects model contains both **fixed effects** and **random effects.** A fixed effect is a parameter that does not vary. Random effects are parameters that are random variables.

![mixed effects example.](mixed_effects_2.png)
It follows the same assumptions are standard OLS regression:
1. linearity
2. no outliers
3. homoscedasticity
4. residuals are normally distributed
5. no multicollinearity

You should consider using a mixed effects model when you have **repeated measures from the same unit of observation.**


```{r}
dragons$bodyLength2 <- scale(dragons$bodyLength, center = TRUE, scale = TRUE)
mixed.lmer <- lmer(testScore ~ bodyLength2 + (1|mountainRange), data = dragons)
summary(mixed.lmer)
```


## Multinomial Logistic Regression (Discrete Choice Model)
#### Based on TO566 class materials

- *Logistic regression*: Models binary outcomes.
- *Multinomial logistic regression*: The model to use when your outcome variable takes one of several options.
- **Example**: A consumer has the option to purchase a product from one of several brands.

### Applications
- Market share and pricing analysis (how would increasing prices affect market share?)
- New technology adoption (what factors influence the adoption of a new technology?)
- Competitive analysis (how do product/service features influence consumer preferences?)

### The Math
- Choices are modeled by utility functions: \( u_i = \alpha_1 + \beta_pPrice_1 + \beta_bBrand_1 \)
  - For each choice, the model coefficients reflect the effect of one unit change in the X variable on the log odds when other
    variables are held constant.
  \[
  \log\frac{p(x)}{(1 - p(x))} = \alpha + \beta_0X + \cdots \beta_nX_n
  \]

Therefore, the probability that a consumer chooses \(j\) (product \(j's\) market share) is:

\[
P[Y_i = j] = p_j = \frac{e^{\beta_0 + \beta_1 x_{j1} + \ldots + \beta_p x_{jp}}}{1 + e^{\beta_0 + \beta_1 x_{j1} + \ldots + \beta_p x_{jp}}}
\]

Some math resources:

- https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture26.pdf
- https://towardsdatascience.com/a-simple-interpretation-of-logistic-regression-coefficients-e3a40a62e8cf
- https://towardsdatascience.com/how-to-interpret-the-odds-ratio-with-categorical-variables-in-logistic-regression-5bb38e3fc6a8

### Example
- Using a dataset of heating choices by household (single family houses in California)
  - dependent variables (choices of heating systems):
    - ec: electric central
    - er: electric room
    - gc: gas central
    - gr: gas room
    - hp: heat pump
  - independent variables (characteristics of choices, characteristics of households)
    - characteristics of the choices (heating options in this case)
      - ic: installation cost (one time cost)
      - oc: operation cost (yearly operating cost)
    - characteristics of households
      - income
      - agehed: age of the head of household
      - rooms: number of rooms in the house
      - region: northern coastal, southern coastal, mountain region, central valley

```{r}
options(warn = -1)
library(mlogit)
library(dfidx)
# Note mlogit requires data in long format
# The multinom function from the nnet package does not require reshaping data
library(caret)
library(tidyverse)

data("Heating") # dataset of heating technologies chosen by households

# Let's look at market shares
prop.table(table(Heating$depvar))
```
```{r}
# long format with two indexes: household index and choice index
heating <- dfidx(Heating, choice="depvar", varying=3:12) # varying=variables that vary across choices
# Create your multinomial regression model
heat_model <- mlogit(depvar ~ ic + oc, data = heating)

# Negative coefficient means less preferred option
# R^2 values tend to be quite small for logit models
summary(heat_model) # baseline: ec

```

Let's examine the coefficients

```{r}

coef(heat_model)

# - ec: electric central
# - er: electric room
# - gc: gas central
# - gr: gas room
# - hp: heat pump

```

Utility function for central electric (ec) $0 – 0.0015 \cdot ic – 0.0070 \cdot oc$

Let's use our model to predict!

```{r}
# Scenario: A new central heating system is developed (ec): saves 25% of electricity (i.e. operating cost is 75%), but the installation cost is $200 more
# If we switch the current ec to this new technology, predict the new market share
H_newEC <- heating
H_newEC [idx(H_newEC,2) == "ec", "oc"] = 0.75 * heating[idx(heating,2) == "ec", "oc"] # take out "ec" rows; change their operating cost (oc) to be 75% of the previous level
H_newEC [ idx(H_newEC,2) == "ec", "ic"] = heating[ idx(heating,2) == "ec","ic"] + 200 # take out "ec" rows; increase their installation cost (ic) by 200
newprob <- predict(heat_model, H_newEC)

print("New Probabilities")
colMeans(newprob) # average of everyone's probabilities for each option

print("Current Probabilities")
prop.table(heat_model$freq)
```
```{r}

```


```{r}
# You can also include household specific variables in the model
heat_model2 <- mlogit(depvar ~ ic + oc | income , heating) # each heating option will have a different coefficent for households with different income level
summary(heat_model2)

```