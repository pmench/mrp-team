{% extends "ppredict/base_materialize.html" %}
{% load static %}
{% block content %}
    <div class="col s12 m9 l10">
      <div id="introduction" class="section scrollspy">
        <div class="container">
            <h1>America's Next Top Model</h1>
            <h3>Demystifying Political Polling</h3>
            <figure>
                <img src="{% static 'ppredict/2020_hexbin.svg' %}" alt="2020 election map" class="responsive-img">
                <figcaption>2020 Presidential election map showing how each state voted.</figcaption>
            </figure>
            <p>
                It's election season! (Is it ever not?) That means it's also election <em>prediction</em> season, where
                legions of pundits, "personalities", and pollsters opine on who will win in November, with varying levels
                of methodological integrity. And although it has a certain cachet to say that Joe Biden has the advantage
                because as a Scorpio Sun in the twelfth house he vibes better with the zeitgeist, most analyst use more,
                ahem, complex methods. But what are those methods? How do they work? How do polls turn into predictions?
                We're here to answer your questions. But first, a quick refresher on U.S. elections.
            </p>
        </div>
      </div>

      <div id="election_explainer" class="section scrollspy">
          <div class="container">
              <h3>All About U.S. Presidential Elections (In 5 Minutes or Less)</h3>
              <p>
                  Here's the weird thing about U.S. presidential elections: The people don't elect the president. One
                  would assume that candidates become president if they win the popular vote, that is if they earn a
                  majority (or plurality) of votes. Instead, when you cast a vote for a candidate, you're actually voting
                  for "electors," who are the people who <em>actually</em> cast votes for a specific candidate.  In most states,
                  the candidate who wins the popular vote in the state gets a group of electors who all cast votes for
                  that candidate (but note that electors aren't necessarily required to vote for their state's winner—in
                  2016, electors cast votes for John Kasich and Colin Powell (Fortier 2020, 9)).
              </p>

              <p>
                  The Constitution empowers state legislatures to decide how to appoint these Electors and currently all
                  states make that decision based on the popular vote, though the mechanics of it vary from state to
                  state (Fortier 2020, 3-6). Suffice to say: Citizens cast their votes for a presidential candidate,
                  which results in certain electors being appointed, who are supposed to follow the popular will in their
                  state and vote for the presidential candidate who earned the most votes in their state. Then, the
                  candidate who wins the majority of votes cast by electors wins the election.
              </p>
              <p>
                  Nationwide, there are 538 electors and each state gets a certain number of electors based on its
                  congressional representation. This leads to a much-criticized aspect of the Electoral College:
                  smaller states wield more electoral power. The size of the House and Senate are fixed so even though
                  the number of Representatives a state gets adjusts based on a state's population, and thus the number
                  of electors each state gets remains relative to its population, the number of people represented by
                  one electoral vote is much greater in populous states.
              </p>
              <figure>
                <img src="{% static 'ppredict/ecollege_pop.gif' %}" alt="ecollege-vs-pop" class="responsive-img">
                <figcaption>
                    The map of the U.S. is first distorted relative to the number of electoral college votes each state 
                    gets (based on the 2020 Census), and then distorts relative to each state's population (2022 ACS 
                    5-Year Estimates).
                </figcaption>
            </figure>
              <p>
                  Confused? Imagine this: You have one pie and have to divide it among groups of people. Each group must
                  get one slice and the size of the slice should be relative to the size of the group. The larger groups
                  get larger slices and the smaller groups get smaller slices. That's only fair right? But you don't have
                  infinite pie (if only!). So even though you can keep the size of slices relative to the sizes of each
                  group, if a big groups keep getting bigger, each person in the big group is going to be getting smaller
                  mouthfuls of pie.
              </p>
              <p>
                  Now think about this: how would candidates' campaign strategies, and their political agendas, change
                  if they had to win the popular vote and not the Electoral College? And what is the purpose of democracy
                  anyway? Is it supposed to enact the will of the majority? Or protect the rights of the minority? While
                  you ponder those questions, we'll turn to the mechanics of political polling and prediction.
              </p>
          </div>
      </div>

      <div id="mrp" class="section scrollspy">
          <div class="container">
              <h3>Polls and Predictions</h3>
              <p>
                  People rely on opinion polling to make election predictions. These polls ask voters how they feel
                  about certain issues and candidates, and sometimes how they intend to vote in particular elections.
                  From those responses, pollsters try to make conclusions about election outcomes. The key challenge, of
                  course, is that it is impossible to get every voter's opinion. If pollsters want to predict how a
                  group of people might vote—the population of an entire county or state, for example—they must find a
                  way to make that prediction based only on a sample of people polled. The standard approach currently is
                  a method with the intimidating name "Multilevel Regression With Poststratification" (or, less 
                  intimidatingly, "Mister P"). Despite the technical-sounding name, the idea behind MrP can be thought of 
                  in a fairly intuitive way: If you want to know how a group will vote, first try to predict how a sample 
                  of that group would vote, then scale up that estimate based on the total population of that group. If, 
                  for example, you asked a group of Latinos in Long Beach, California, whether they prefer dogs or cats, 
                  and 60% prefer cats, you can scale up and predict that in the upcoming (extremely made-up) ballot 
                  initiative to make cats the official pet of Long Beach, 120,000 of Long Beach's 200,000 Latinos would 
                  back that initiative (we should emphasize that this is an extremely simplified, extremely fake example).
              </p>
              <p>
                  Now perhaps a few problems with this approach might occur to you. For example, one assumption is that 
                  groups of people tend to vote the same way. And, indeed, deciding how to group people is one of the key
                  decisions when creating prediction models. Another assumption is that you actually know the composition 
                  of the total population. If there were only 10,000 Latinos in Long Beach, you would be wildly overestimating
                  support for cats! And, of course, you're assuming that the people you <em>did ask</em> are representative 
                  of the people you <em>didn't ask</em>.
              </p>
              <p>
                  While MrP doesn't magically solve these problems, it does have some magic. Specifically, MrP is clever
                  in the way it pools groups of people together. Working from a description of MrP by Joseph Ornstein,
                  let's think about how MrP would work with our invented scenario (2023). If you took a statewide poll
                  of people's opinion of cats, the sample of Latinos in Long Beach is likely to be quite small, and thus
                  unlikely to be representative of the opinion of Latinos in Long Beach. But you can use the results of
                  similar groups in other areas to adjust your prediction about the opinion of Latinos in Long Beach.
                  Further, you can do these kinds of adjustments at different levels. You can use demographic
                  characteristics, such as race and gender, and higher-level group characteristics such as income to
                  create different pools of people, using larger group estimates to scale and adjust estimates for
                  smaller groups in your data. In this way, you can squeeze as much information out of your data as
                  possible and make more accurate predictions!
              </p>
          </div>
      </div>

      <div id="machine_learning" class="section scrollspy">
          <div class="container">
            <h3>Hark! Machine Learning!</h3>
              <p>
                  You have no doubt heard of machine learning. Machine learning, or ML, is a kind of artificial intelligence
                  where a computer identifies patterns in data and creates a mathematical model from those patterns to
                  make predictions. Imagine if an ML model learned that data generated by a specific phenomenon over time
                  lay pretty much along a single line. It might then model this with the simple function \(y = mx + b\),
                  where \(x\) is the time of an observation. That means if you wanted to know what data would look like
                  at a specific point in time or over time, and the ML model was correct about the function, you could
                  get good predictions by feeding in the new time data as \(x\) variables.
              </p>
              <p>
                  ML's power comes from its ability to model highly complex interactions and very large datasets, things
                  that are not well described by \(y = mx + b \). And crucially, people assess and refine ML models in
                  various ways to improve their accuracy. Now, can you think of something that has highly complex
                  interactions and lots of data? If you thought of polling and political predictions, you are correct!
                  Could ML be combined with MrP to create better predictions? Let's find out!
              </p>
              <figure>
                  <img src="{% static 'ppredict/pipeline.svg' %}" alt="prediction_pipeline" class="responsive-img">
                  <figcaption>Full workflow for a combined MrP and ML prediction model.</figcaption>
              </figure>
              <p>
                  Let's break down what's happening with this pipeline. First, we select polling data from a past election
                  whose outcome we know. In this case, we're using polling data from the 2020 U.S. presidential election.
                  Selections from the data are used to build the MRP and ML models. Note that the output of the MrP
                  model—the probability that someone will vote for either Joe Biden or Donald Trump—is fed into the ML
                  model. The idea is that the MrP's prediction can give the ML a "hint" about an individual's voting
                  preference and combine with the more complex interactions between voter beliefs and demographic data
                  detected by the ML model. This training process generates a mathematical model that can be fed new
                  data to make predictions. For example, new polls can be used to generate predictions about who will win
                  the 2024 presidential election. But first, let's see how we did with our prediction for 2020. Because
                  we actually know the outcome of that election, we can use it to test whether our model can make good
                  predictions.
              </p>
              <figure>
                  <img src="{% static 'ppredict/2020_compare@2x.png' %}" alt="2020_prediction" class="responsive-img">
                  <figcaption>
                      Comparison between predicted 2020 election results and the actual results. You can explore the
                      code behind these models on our project's <a href="https://github.com/pmench/mrp-team" target="_blank">
                      GitHub repository</a>.
                  </figcaption>
              </figure>
              <p>
                  Well, it's not perfect. In fact, we can calculate that its accuracy is 68%
                  \[
                  \frac{\text{number of states ML accurately predicted}}{\text{total number of states}} = \frac{35}{51} \approx 68\%
                  \]
                  As a comparison, we also calculated how well our MrP model did on its own, without being used as an
                  input to the ML model.
                  \[
                  \frac{\text{number of states MrP accurately predicted}}{\text{total number of states}} = \frac{39}{51} \approx 76\%
                  \]
                  Based on these results, our models perform better than random guessing but neither models is
                  particularly great, and the ML-enhanced model actually performs worse than the standard MrP approach.
                  So what went wrong?
              </p>
          </div>
      </div>

      <div id="perils" class="section scrollspy">
          <div class="container">
            <h3>The Perils of Prediction</h3>
              <p>
                  Predicting elections is hard and even the best models make incorrect predictions. Remember, the
                  foundation of these models—the data used to create most models—are polls. But it's actually unclear to
                  what extent polls can accurately capture what will happen at the ballot box. At the most basic level,
                  polls generally assume that respondents are honest in their responses (or that dishonesty is rare
                  enough that a sufficiently large survey will compensate for it) but that's not necessarily true.
                  Additionally, it is possible that polls are better at capturing voter sentiment rather than voter
                  intention; just because a voter does not like a candidate does not mean they won't vote for that candidate.
                  And, of course, every election is different and dynamic. Models built on last year's or last month's
                  (or last week's!) data can easily go astray when current events render them obsolete. There are also
                  endemic problems with polling data. MrP uses demographic categories to predict how groups will vote,
                  which means it must conform to the categories devised by demographers. The U.S. Census Bureau has known
                  issues with its demographic categories. For example, over 3.5 million Americans of Middle Eastern and
                  North African (MENA) descent have long been labeled as “white” by the Census, even though they are
                  culturally and ethnically distinct (Zraick et al., 2024). If these groups have distinctly different
                  political beliefs but are all grouped together, it will lead the model astray. Last, some elections
                  are so close that it simply becomes impossible for a model to accurately predict—in the 2016
                  presidential election, Donald Trump won Michigan by just 11,000 votes (The New York Times, 2017)!
              </p>
              <p>
                  When making predictions, the adage "garbage in, garbage out" rules the day. No matter how much fancy
                  math you use, models are ultimately limited by the data used to build them. And in the world of political
                  prediction, good data is big business. The 2012 election saw approximately $3.6 billion dollars—more
                  than half of the total amount spent on races—directed to consulting firms (Sheingate 2016, 1). This
                  means political consultants and campaigns can be protective of their data so what's available to the
                  public is limited and sometimes in formats that make it difficult or impossible to use for modeling.
                  We built our models from publicly available political opinion surveys and so were limited in what data
                  we could acquire. We tried different methods to improve the accuracy of our models but our results
                  kept pointing in one direction—the greatest gains in accuracy came from incorporating more data.
              </p>
              <p>
                  So what does all this mean for interpreting polls and political predictions? We'd like to encourage
                  skepticism. Remember:
              <ol>
                <li>The quality of data sets a limit on the accuracy of predictions.</li>
                <li>Polling data has fundamental limitations and flaws.</li>
                <li>The world is full of surprises! Public opinion can change far more rapidly than pollsters and political
              consultants can update their models.</li>
          </ol>
              </p>
          </div>
      </div>

    <div id="election_2024" class="section scrollspy">
        <div class="container">
            <h3>The 2024 Election</h3>
            <p>
                Okay, if you've made it this far, it's time for a test. Here's what our model predicted for the 2024 
                presidential election. Its prediction is based on a January 2024 Reuters/Ipsos poll of approximately 4,000
                adults. Do you trust our prediction?
            </p>
            <figure>
                <img src="{% static 'ppredict/election_2024@2x.png' %}" alt="2024_prediction" class="responsive-img">
                <figcaption>
                    Our ML-enhanced model's state-by-state prediction for the 2024 election. It predicts a win for 
                    candidate Donald Trump by a margin of 339-199 in the electoral college.
                </figcaption>
            </figure>
        </div>
    </div>

      <div id="about" class="section scrollspy">
          <div class="container">
            <h3>About</h3>
              <p>
                  PollRBear was created by Bella Karduck, Haley Johnson, Rohit Maramraju, and Philip Menchaca in partial
                  fulfillment of the requirements for the degree of Master of Science in Information, Data Science, at
                  the University of Michigan, Ann Arbor. It is intended to provide an accessible introduction to methods
                  used for political polling and prediction. Graphics were generated with Python and R, and this website
                  was built with the Django and Materialize CSS frameworks.
              </p>
          </div>
      </div>

    <div id="sources" class="section scrollspy">
        <div class="container" style="font-size: 12px; text-indent: -1.5em; padding-left: 1.5em">
            <h3>Sources</h3>
            <ul>
                <li>Bureau, US Census. 2020. “American Community Survey 5-Year Data (2009-2022).” https://www.census.gov/data/developers/data-sets/acs-5year.html.</li>
                <li>Clarke, Harold, and Marianne Stewart. 2020. “COMETrends November 2020 Election Survey Data.” https://cometrends.utdallas.edu/data-and-questionnaires/.</li>
                <li>Fortier, John C., ed. 2020. After the People Vote: A Guide to the Electoral College. Fourth edition. Washington, DC: American Enterprise Institute Press.</li>
                <li>Hill, Andrew. n.d. “U.S. States Hexgrid.” CARTO. Accessed April 7, 2024. https://team.carto.com/u/andrew/tables/andrew.us_states_hexgrid/public/map.</li>
                <li>Institute, Monmouth University Polling. 2021a. “Monmouth University National Poll, Number 213.” UNC Dataverse. https://doi.org/10.15139/S3/6TEXFU.</li>
                <li>———. 2021b. “Monmouth University National Poll, Number 218.” UNC Dataverse. https://doi.org/10.15139/S3/GCQAAI.</li>
                <li>———. 2021c. "Monmouth University Georgia Poll, Number 220." UNC Dataverse. https://doi.org/10.15139/S3/QPC3PL. </li>
                <li>———. 2021d. “Monmouth University National Poll, Number 222.” UNC Dataverse. https://doi.org/10.15139/S3/IVDJPS.</li>
                <li>National Archives and Records Administration. 2021. “2020 Electoral College Results.” National Archives. April 16, 2021. https://www.archives.gov/electoral-college/2020.</li>
                <li>Ornstein, Joseph T. 2023. “Getting the Most Out of Surveys: Multilevel Regression and Poststratification.” In Causality in Policy Studies: A Pluralist Toolbox, edited by Alessia Damonte and Fedra Negri, 99–122. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-12982-7_5.
</li>
                <li>Reuters, Reuters/Ipsos Large Sample Survey 1: January 2024, Ipsos, (Cornell University, Ithaca, NY: Roper Center for Public Opinion Research, 2024), Dataset, DOI: 10.25940/ROPER-31120717.</li>
                <li>Ruggles, Steven, Sarah Flood, Matthew Sobek, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Renae Rogers, and Megan Schouweiler. 2024. IPUMS USA: Version 15.0 [American Community Survey 5-Year]. Minneapolis, MN: IPUMS. https://doi.org/10.18128/D010.V15.0. </li>
                <li>Schaffner, Brian, Stephen Ansolabehere, and Sam Luks. 2022. “Cooperative Election Study Common Content, 2020.” Harvard Dataverse. https://doi.org/10.7910/DVN/E9N6PH.</li>
                <li>Sheingate, Adam D. 2016. Building a Business of Politics: The Rise of Political Consulting and the Transformation of American Democracy. Oxford University Press.</li>
                <li>The New York Times. 2017. “Michigan Election Results 2016,” August 1, 2017, sec. U.S. https://www.nytimes.com/elections/2016/results/michigan.
</li>
                <li>Zraick, Karen, Allison McCann, Sarah Almukhtar, Yuliya Parshina-Kottas, Robert Gebeloff, and Denise Lu. 2024. “No Box to Check: When the Census Doesn’t Reflect You.” The New York Times, February 26, 2024, sec. U.S. https://www.nytimes.com/interactive/2024/02/25/us/census-race-ethnicity-middle-east-north-africa.html.
</li>
            </ul>
        </div>

    </div>
    </div>
{% endblock %}